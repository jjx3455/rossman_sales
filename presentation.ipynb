{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# File structure of the model\n",
    "\n",
    "The model is composed of the following files:\n",
    "<ol>\n",
    "<li> a main file containing the data input, and the fitting and prediction of the model. The data loading is described in the 'Data input' section.</li>\n",
    "\n",
    "<li> a data cleaning file where a data cleaning class is defined. The effect of the data cleaning class is described in 'Data cleaning section'</li>\n",
    "\n",
    "<li> a model file containing the actual model and its parameters. The content of the model is descrived in the 'Model' section.</li>\n",
    "</ol>   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "from models import define_pipelines\n",
    "from models import single_run"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data input\n",
    "<ol>\n",
    "<li> The data are loaded.</li>\n",
    "\n",
    "<li> We split the sales from the data. We obtain three DataFrames:\n",
    "    <ol>\n",
    "    <li> Training data for the store: 'Date', 'Store', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday'</li>  \n",
    "    <li> Store data: 'Store', 'StoreType', 'Assortment', 'CompetitionDistance',\n",
    "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval'</li>\n",
    "    <li> The target data: a Dataframe of one columns: 'Sale'</li>\n",
    "    </ol>\n",
    "</li>\n",
    "</ol>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TRAINING_DATA = 'data/train.csv'\n",
    "HOLDOUT_DATA = 'data/holdout.csv'\n",
    "STORE_DATA = 'data/store.csv'\n",
    "TEST_DATA = '' # input the path of the test data file here\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "CORES = -1\n",
    "\n",
    "try:\n",
    "    df_test = pd.read_csv(TEST_DATA)\n",
    "except FileNotFoundError:\n",
    "    print('Test data file not found, using holdout as validation set')\n",
    "    df_test = pd.read_csv(HOLDOUT_DATA)\n",
    "    df_train = pd.read_csv(TRAINING_DATA)\n",
    "else:\n",
    "    print('Test data loaded, using full training data for model training')\n",
    "    df_train = pd.concat([\n",
    "        pd.read_csv(TRAINING_DATA),\n",
    "        pd.read_csv(HOLDOUT_DATA)\n",
    "    ])\n",
    "finally:\n",
    "    df_store = pd.read_csv(STORE_DATA)\n",
    "\n",
    "X_train = df_train.drop(columns='Sales')\n",
    "y_train = df_train.loc[:, 'Sales']\n",
    "X_val = df_test.drop(columns='Sales')\n",
    "y_val = df_test.loc[:, 'Sales']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data cleaning\n",
    "\n",
    "<ol>\n",
    "<li> We merge an inner merge on the store columns, between the training data and the store data. </li>\n",
    "\n",
    "\n",
    "<li> Rows with empty sales or store id are dropped. </li>\n",
    "\n",
    "<li> We drop the rows for which the sales are zeros. </li>\n",
    "\n",
    "<li> At inspection of the data, we observe that the 'StateHoliday' has a class defined by two distinct parameters: an integer 0 and a string 0. This is corrected. </li>\n",
    "\n",
    "<li> For the date: we add three new columns containing week day, day, month and year. The columns 'DayofWeek' is redun dant, and dropped. The 'Date' columns is ultimately dropped. </li> \n",
    "\n",
    "<li> We perform one hot encoding on the following parameters:\n",
    "     'StateHoliday', 'Assortment', 'SchoolHoliday'\n",
    "   For those columns containing Nan, a nan type columns is created. </li>\n",
    "\n",
    "\n",
    "<li>  We introduce the follwing features\n",
    "    <ol>\n",
    "    <li> for each row, the median store sale of the corresponding store id of the row is added. </li>\n",
    "    <li> for each row, the median store standard deviation of the corresponding store id of the row is added. </li>\n",
    "    <li> for each row, the  store type mean sale of the store type of the corresponding row is added. </li>\n",
    "    <li> for each row, the store stype standard deviation type of the store type of the corresponding row is added. </li>\n",
    "     <li>   Ultimately, 'Store' id and 'Storetype' columns are dropped. </li>\n",
    "     </ol>   \n",
    "\n",
    "    \n",
    "\n",
    "<li> The 'CompetitionDistance' column has missing value, and is filled in median. </li>\n",
    "\n",
    "<li> The 'Promotion' column has missing value, and is filled with the mininum of the remaining value.</li>\n",
    "\n",
    "<li> Ultimately, the following columns are dropped:\n",
    "'Store', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear','Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'Date', 'Open',\n",
    "'StoreType'. </li>\n",
    "\n",
    "<li> The output of the cleaning is two dataframes with identical indices. The feature daraframe has the following columns:\n",
    "\n",
    "'Promo', 'Sales', 'CompetitionDistance', 'Promo2', 'day', 'month',\n",
    "       'year', 'weekday', 'median_sales', 'std_sales', 'type_median',\n",
    "       'type_std', 'mean_sales', 'StateHoliday_b', 'StateHoliday_c',\n",
    "       'StateHoliday_no', 'StateHoliday_nan', 'Assortment_b', 'Assortment_c',\n",
    "       'Assortment_nan', 'SchoolHoliday_1.0', 'SchoolHoliday_nan'</li>\n",
    "   \n",
    "    \n",
    "</ol>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from data_cleaning import DataCleaning\n",
    "\n",
    "cleaning_settings = dict(\n",
    "    hot_encoded_columns=[\n",
    "        'StateHoliday',\n",
    "        'Assortment',\n",
    "        'SchoolHoliday',\n",
    "    ],\n",
    "    dropped_columns=[\n",
    "        'Store',\n",
    "        'CompetitionOpenSinceMonth',\n",
    "        'CompetitionOpenSinceYear',\n",
    "        'Promo2SinceWeek',\n",
    "        'Promo2SinceYear',\n",
    "        'PromoInterval',\n",
    "        'Date',\n",
    "        'Open',\n",
    "        'StoreType',\n",
    "    ],\n",
    "    filled_in_median=[\n",
    "        'CompetitionDistance',\n",
    "    ],\n",
    "    filled_in_mode=[\n",
    "        'Promo',\n",
    "    ],\n",
    "    target=[\n",
    "        'Sales',\n",
    "    ],\n",
    ")\n",
    "\n",
    "cleaning = DataCleaning(\n",
    "    store=df_store,\n",
    "    hot_encoded_columns=cleaning_settings['hot_encoded_columns'],\n",
    "    dropped_columns=cleaning_settings['dropped_columns'],\n",
    "    filled_in_median=cleaning_settings['filled_in_median'],\n",
    "    filled_in_mode=cleaning_settings['filled_in_mode'],\n",
    "    target=cleaning_settings['target'],\n",
    ")\n",
    "\n",
    "X_train_clean, y_train_clean =\\\n",
    "    cleaning.cleaning(X_train, y_train, training=True)\n",
    "X_val_clean, y_val_clean =\\\n",
    "    cleaning.cleaning(X_val, y_val, training=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Definition\n",
    "\n",
    "<li> We opt for a boosted trees model (XGBRegressor) as this model showed to be less\n",
    "prone to over-fitting compared to our alternative (a random forest). </li>\n",
    "<li> After iterating over the feature selection and transformation we settle on the following features:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train_clean.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<li> A basic grid-search over some key hyper-parameters showed that 500 shallow trees with 3 levels and a learning rate of 0.2 performed best."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xg_settings = dict(\n",
    "    n_estimators=500,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.2,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=CORES,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define our pipeline (which consists of just the model itself), as scaling\n",
    "showed no positive effect in our iterations:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipes = define_pipelines(xg_settings)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then run the model, which returns a self-evaluation:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "__i = single_run(pipes, X_train_clean, y_train_clean,\n",
    "           X_val_clean, y_val_clean, X_train, X_val)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('minicomp-rossman': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "5d19314d9020e914d744f2de230b7e704f3805b9660f27ba32ac517cf1b2bef2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}